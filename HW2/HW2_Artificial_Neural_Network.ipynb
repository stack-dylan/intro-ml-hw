{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2\n",
    "\n",
    "Dylan Ang\n",
    "\n",
    "For ECS 171 - Fall 2022 - E. Solares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhTEdM56b3-n"
   },
   "source": [
    "### Download the data\n",
    "Let's download and uncompress our data and images here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMPNGHDXbxEw",
    "outputId": "8ed99e11-ac95-400c-9c4b-cad7c6738855"
   },
   "outputs": [],
   "source": [
    "# We need to first download the data here:\n",
    "file_id = '1-mQuJbHvSq9nRugy96dr0aB7isfzaxMZ'\n",
    "file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id\n",
    "!wget -O data.zip --no-check-certificate \"$file_download_link\"\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O9KZgKoYnaP"
   },
   "source": [
    "# Artificial Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fASdT6lKYnaS"
   },
   "source": [
    "### Contents\n",
    "1. Artificial neural networks (ANNs) and their link to Logistic regression\n",
    "2. Generalization of multi layer networks\n",
    "3. Step by step implementation of our ANN class\n",
    "4. Does it do what it is supposed to do?\n",
    "    1. linear regression: 25 all over again!\n",
    "    2. Another go at our Titanic dataset\n",
    "    3. Complex fit of flowers\n",
    "    4. Andrew Ng's cat vs non-cat classifying\n",
    "5. Round up\n",
    "**Apendix:**\n",
    "- Data creation/processing for Rose / Cats\n",
    "- Crude version of a hyperparameter sweep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTteqw79YnaS"
   },
   "source": [
    "## Artificial neural networks (ANNs) and their link to Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJmMNloLYnaS"
   },
   "source": [
    "When we would ask a random person about Machine Learning, there is a big chance that neural networks are mentioned. Not only does the terminology play with our imagination, but these mathematical structures have also proven themselves to solve complex tasks. Many of you have probably seen such a network and it almost radiates simplicity:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=10yZYlGrKUh0uFDQ862TphkNjT3EBQBIS\" alt=\"Artificial Neural Network example\" width=\"500\" style=\"display: block; margin: 0 auto\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60kRcxs2YnaT"
   },
   "source": [
    "In this simple diagram, we show an artificial neural network (ANN), or just simply neural network (NN), which has three layers. By convention, we do not count the input layer, and later we see that this layer in the diagram represents the input data you feed into the NN. This particular NN has two hidden layers. While I am not sure why it exactly is called a hidden layer, I can imagine that one reason could be that these layers are 'hidden' from the user. A user inputs data through the input data and gets results from the output layer, therefore, not interacting with the hidden layers. The user would not see if there is just a single layer, two layers, or even ten layers. The output however, is an actual layer, in our case a single neuron, which 'collects' the results from the previous hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDF1aLoTYnaT"
   },
   "source": [
    "A logistic regression can be seen as the tiniest possible NN, with just a single layer, consisting of a single neuron. A diagram of a logistic regression can look like this:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1W-qiH9eroJJUZR82Qy81liXnBK_knp37\" alt=\"Artificial Neural Network example\" width=\"400\" style=\"display: block; margin: 0 auto\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDdqYzipYnaT"
   },
   "source": [
    "Let's do a short recap on this logistic model. First, we massaged the input data in such a way that the individual features ($x_1, x_2, x_3$) are in the rows of our input vector $X$, and the columns are the examples (training samples). This input vector $X$ is then forwarded to our single neuron, which can be divided in two parts. First, a linear operation (linear regression), $z = X W + b$, followed by an activation function $A = g(z)$. In our previous tutorial we performed binary logistic regression, which uses the sigmoid activation function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. Here we generalize this by using $g(z)$ to indicate an activation function we have to define. As we will learn today, there are many other activation functions to choose from.\n",
    "\n",
    "Before we can calculate the activation function we need to calculate the linear part $z$. This is the inner product of the weights vector $W$ and the input vector $X$, with the bias term $b$ added. The bias term $b$ and the weights vector $W$ (consisting of $w_1, w_2, w_3$) are the trainable parameters of this system. Each trainable weight ($w_1, w_2, w_3$) corresponds to an input feature ($x_1, x_2, x_3$) and represents the 'weight' this feature is adding to the problem. These weights are drawn inside the output layer frame, meaning that they are linked to this layer (inside this single neuron).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB04B12gYnaU"
   },
   "source": [
    "While logistic regression is a great tool, in can only divide the parameter space in a line, at least in the from we have presented here. For example, if you have two features $x_1$ and $x_2$, which will be used to predict $y$, a logistic classifier is only able to have a linear boundary between the two parameters. If this is not completely clear, we will have an example to show this problem.\n",
    "\n",
    "To have the system predict more complex relations, we can add more neurons to a layer and even add more layers to our network. Each of this neuron is a kind of logistic regression unit and many of these combined can predict highly non-linear relations. I say kind of, because in a logistic regression we typically use the sigmoid function, while in a NN, many other activation function perform much better.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1E1iyQlEDcd7ARsnTS-Tgm3JaK3lGwfKB\" alt=\"Artificial Neural Network example\" width=\"500\" style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Before we will start to generalize a NN and the used layers, observe how the different neurons are connected. You will notice that each node is connected to all nodes of the next layer. This is called densely connected (sometimes fully connected) and such a layer is often called a 'Dense layer'. Now, lets try to define such structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KliNfAWUYnaV"
   },
   "source": [
    "## Generalization of multi layer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPsVuOUAYnaV"
   },
   "source": [
    "Before we can start to develop our system, lets try to first generalize the steps required. The goal of this tutorial is to create a general NN class in which we can add arbitrary number of layers, containing arbitrary number of neurons. Later we will test this structure on various problems with varying complexity.\n",
    "\n",
    "When we think of the input layer, we have already discovered that it is not an actual layer of the neural network, but the training data, massaged in the right form. Therefore, the input 'layer' is not part of our architecture.\n",
    "\n",
    "The neural network will however consist of an arbitrary number of layers. These will be structures that behave in a similar fashion and are sequetial to each other. When doing the forward pass, we will loop through all the layers and use the output of the previous layer as input for the current layer. The output of the final layer, also called the final activation $A$ is the output of the neural network. This means that, if the neural network is used to predict binary values, the output has to be converted (or rounded) to the actual predictions $\\hat Y$.\n",
    "\n",
    "To do one pass of gradient descent, we need to have the weights. For this, we need to calculate the gradient of the loss function with respect of $W$ and $b$ for each layer. To calculate the gradients, we can do a clever trick and cache the values of $z$ and $A$ during the forward pass. We need to do some bookkeeping here the most mathematically heavy part is quite doable.\n",
    "\n",
    "After all gradients are calculated the weights are then updated using a simple gradient descent step. To make is visual I have created the following diagram:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1jLFZ28_VOCKXmYkaIhVK070gNOuQOSf7\" alt=\"Artificial Neural Network example\" width=\"700\" style=\"display: block; margin: 0 auto\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxObOFjpOu4G"
   },
   "source": [
    "Here the forward pass calculates our loss and is used in the backward pass to calculate updated weights based on alpha. Next we calculate our loss again in the forward pass which then is used again in the next GD step to update weights based on the gradient descent step based on alpha. We do this until we minimize our loss past a certain threshold or after a certain number of iterations. We will now go into detail of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaJnutauYnaV"
   },
   "source": [
    "All layers in the network are fundamentally the same. They can differ in the amount of nodes or the activation function, but there is no structural difference between the first layer $L_1$ and the output layer $L_o$. \n",
    "\n",
    "As we will be creating only fully connected Dense layers, we can create a single structure to do the hard work. In this structure we have to do the math for the forward pass, the backward pass, and the update. The input of each layer is generally the output of the previous layer. Obviously, for the first layer, this is the input vector. For the backward pass, we sequentially go through the network in the reverse direction. However, here we need to do an addtional step. We need to calculate the gradients with respect to the defined loss function. For this we need to input the true labels $Y$ and the predicted output $A$ during the backward pass. After this, we again have that the previous gradient is the input for the next layer (remember: we are going backwards). Each layer handels the backwards pass in the same way:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=13IN6rIkiSdCuT8lT8BSasCj4wdA_spGW\" alt=\"Artificial Neural Network example\" width=\"600\" style=\"display: block; margin: 0 auto\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr62GIGZYnaV"
   },
   "source": [
    "For each layer, in the forward pass, in expects the output of the previous layer (or $X$). This is then cached for use during the backward pass. We calculate the linear part $z$ with one single vectorized inner product calculation for all neurons and add the bias term. We also cache this value of $z$ for the backward pass. Finally, we will calculate the activation function and pass the result to the next layer (or this is the output if it is the final layer).\n",
    "\n",
    "The backward pass is just the forward pass in reverse, but expects the previous gradient as an input. We calculate the gradients, using the cached values of $A_p$ and $z_p$. As a last step we calculate $dA_p$ which will be the input to the next layer. I do not want to focus to much on the actual differentials, but will explain them a bit more in the code. If you really want to know how these differentials are calculated, I would suggest to get a pen and paper and try to calculate them. They are not hard and [Wolfram Alpha](https://www.wolframalpha.com/) can help you :-).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb3TFMsiYnaV"
   },
   "source": [
    "If it is not completely clear what each step is doing, do not worry to much about it. I will try to explain each step during the coding part. To have an idea, let us summarize what we will be making:\n",
    "\n",
    "#### Dense Layer Class\n",
    "- structure to hold an arbitrary number of nodes\n",
    "- will have various activation functions\n",
    "- will perform the forward pass through the single layer (z and A)\n",
    "- will perform the backward pass for the layer\n",
    "- will perform the update step for the layer\n",
    "\n",
    "#### Neural Network Class\n",
    "- structure to hold an arbitrary number of layers\n",
    "- will perform the forward pass sequentially through all layers\n",
    "- will do the cost calculation for various loss functions\n",
    "- will perform the backward pass and calculate all gradients\n",
    "- will do an update (a step of gradient descent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UShVaJiHYnaW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-xm3KnxYnaW"
   },
   "source": [
    "## Step by step implementation of our ANN class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYPvx5awYnaW"
   },
   "source": [
    "Now lets start to implement our NN classes. First import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zL1c0GPfYnaW"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cryqx1XlYnaX"
   },
   "source": [
    "Before we can define our main NN class, we need to define our Dense layer class. As I want to do some error checking, we will first define some custom Exceptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAemRcfmYnaX"
   },
   "outputs": [],
   "source": [
    "class ActivationDoesNotExist(Exception):\n",
    "    \"\"\"Valid activations are sigmoid, tanh, and relu, provided as a string\"\"\"\n",
    "    pass\n",
    "\n",
    "class InputDimensionNotCorrect(Exception):\n",
    "    \"\"\"Need to specify input dimension, i.e. input shape into the first layer\"\"\"\n",
    "    pass\n",
    "\n",
    "class LossFunctionNotDefined(Exception):\n",
    "    \"\"\"Loss function in cost() method not defined\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtY-qXimYnaX"
   },
   "source": [
    "These exception extend Python's default Exception class and now can be used with the 'raise' keyword to create an exception that tells the user what the mistake is. If you systems get larger, it is good practice to have meaningfull errors. Another great way is using the logging module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXbJ-bRVYnaX"
   },
   "source": [
    "Now we can begin with our new class called DenseLayer. This class takes an constructor with two required parameters:\n",
    "- inputDimension, i.e. the number of features of the input vector, or the number of units from the previous layer\n",
    "- units, i.e. the number of neurons in this layer\n",
    "There are two more optional parameters:\n",
    "- activation: here you can indicate what activation the layer should use. Defined are sigmoid, relu, tanh, and ''. The empty string indicates no activation, meaning we just have a regression.\n",
    "- randomMultiplier is the value which the random weights are multiplied. Generally 0.01 is fine, but sometimes tweaking this number can help.\n",
    "\n",
    "As there can me different activation functions and we do not want to chech which activation we have using an if-statement, we make a reference to the used activation functions in the init statement.\n",
    "\n",
    "In the initialize method, the weights are initialized. Notice that the number of neurons nh (units) are in the rows, and the number of input features nx are the columns. This is required to make our dot product work later.\n",
    "\n",
    "Another thing I learned recently is the use of 'self' in Python. While I though I understood the concept, I did not fully understand the concequences. The definition of the class is separated from the values of each instance. These values are stored in the self object, i.e. the object of the instance. In other programming languages it is common to define types it the class itself. However, in Python you have to define these in the init method. When not doing this, the variable is the same object in all your instances and you can get weird results. I just found this [blog-post](https://towardsdatascience.com/python-pitfall-mutable-default-arguments-9385e8265422) where Don Cross has a very clear explanation.\n",
    "\n",
    "\n",
    "```python\n",
    "class DenseLayer:\n",
    "    def __init__(self, inputDimension, units, activation='', randomMultiplier=0.01):\n",
    "        self.weights, self.bias = self.initialize(inputDimension, units, randomMultiplier)\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = activation\n",
    "            self.activationForward = self.sigmoid\n",
    "            self.activationBackward = self.sigmoidGrad\n",
    "        elif activation == 'relu':\n",
    "            self.activation = activation\n",
    "            self.activationForward = self.relu\n",
    "            self.activationBackward = self.reluGrad\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = activation\n",
    "            self.activationFunction = self.tanh\n",
    "            self.activationBackward = self.tanhGrad\n",
    "        elif activation != '':\n",
    "            raise ActivationDoesNotExist\n",
    "        else:\n",
    "            self.activation = 'none'\n",
    "            self.activationFunction = self.linear\n",
    "            self.activationBackward = self.linear\n",
    "    \n",
    "    def initialize(self, nx, nh, randomMultiplier):\n",
    "        weights = randomMultiplier * np.random.randn(nh, nx)\n",
    "        bias = np.zeros([nh, 1])\n",
    "        return weights, bias\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHqOChxmYnaY"
   },
   "source": [
    "Next we define all used activation functions. The sigmoid we already know from the previous Tutorial, however we also include tanh and relu, which are also very common activation functions.\n",
    "\n",
    "Tanh or hyperbolic tangent in full is a function which is similar to the sigmoid, however it maps all real values between -1 and +1. It was very popular, before the Relu function made its way to the stage. \n",
    "\n",
    "Relu, which stands for Rectified linear unit, is probably the most popular activation function. It is fast to calculate and has often better results than the Tanh function. Therefore, if you are not sure, the Relu function is a great start. The Relu function maps all values smaller than 0 to zero and all values larger than 0 as the value itself. \n",
    "\n",
    "The last activation function we will introduce is called the linear function. This is the same as not having an activation function, and is just a placeholder. What comes in, comes out and we use it to test our previous excersise on Linear regression.\n",
    "\n",
    "The backward pass needs the differentials of all these functions, which are provided with the Grad suffix. Feel free to check if these differentials are correct. One additional step for each Grad calculation is that we multiply the input gradient (dA0 with the calculated gradient. This is required because we use the [chain rule](https://en.wikipedia.org/wiki/Differentiation_rules#The_chain_rule) (continously through back propagation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSxl13x4YnaY"
   },
   "source": [
    "```python\n",
    "    def sigmoid(self, Z):\n",
    "            A = 1 / (1 + np.exp(-Z))\n",
    "            return A\n",
    "        \n",
    "    def sigmoidGrad(self, dA):\n",
    "            s = 1 / (1 + np.exp(-self.prevZ))\n",
    "            dZ = dA * s * (1 - s)\n",
    "            return dZ\n",
    "    \n",
    "    \n",
    "    def relu(self, Z):\n",
    "            A = np.maximum(0, Z)\n",
    "            return A\n",
    "        \n",
    "    def reluGrad(self, dA):\n",
    "            s = np.maximum(0, self.prevZ)\n",
    "            dZ = (s>0) * 1 * dA\n",
    "            return dZ \n",
    "\n",
    "        \n",
    "    def tanh(self, Z):\n",
    "            A = np.tanh(Z)\n",
    "            return A\n",
    "\n",
    "    def tanhGrad(self, dA):\n",
    "            s = np.tanh(self.prevZ)\n",
    "            dZ = (1 - s**2) * dA\n",
    "            return dZ\n",
    "\n",
    "\n",
    "    def linear(self, Z):\n",
    "        return Z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWvqxVobYnaY"
   },
   "source": [
    "Next, we define the forward propagation step, which should look very familiar:\n",
    "```python\n",
    "    def forward(self, A):\n",
    "        Z = np.dot(self.weights, A) + self.bias\n",
    "        self.prevZ = Z\n",
    "        self.prevA = A\n",
    "        A = self.activationForward(Z)\n",
    "        return A\n",
    "```\n",
    "\n",
    "We first calculate the linear part. Store the values for Z and A for later use in the back propagation and next apply the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxnfkWXWYnaY"
   },
   "source": [
    "Before starting our backpropstep we need to define our loss function. For logistic regression it's done with the binary cross entropy, for which I will simply give the equation. \n",
    "\\begin{equation}\n",
    "loss = -\\frac{1}{m}\\sum_{i=1}^{m}y\\log(A)+(1-y)\\log(1-A)\n",
    "\\end{equation}\n",
    "\n",
    "Now for the backwards pass, We would need to differentiate the Loss function with $W$ and $b$. Not to  bore you guys, \n",
    "I have provided these functions:\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{\\partial loss}{\\partial W} = \\frac{1}{m} \\sum_{i=1}^m  X(A - Y)^T \\\\\n",
    " \\frac{\\partial loss}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (A - y)\n",
    "\\end{equation}\n",
    "\n",
    "In Python, this looks like this:\n",
    "\n",
    "The backward propagation looks again similar to the previous tutorial, however we have split the backward activation part to the activation functions itself. Also this function expects to get dA, which is a more generalized form for multiple layers. In our previous single layer example we combined the differential of the loss function in this step. Now this step is in our NN class and not in each layer, as it is only required at the last layer.\n",
    "\n",
    "```python\n",
    "  def backward(self, dA):\n",
    "        dZ = self.activationBackward(dA)\n",
    "        m = self.prevA.shape[1]\n",
    "        self.dW = 1 / m * np.dot(dZ, self.prevA.T)\n",
    "        self.db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        prevdA = np.dot(self.weights.T, dZ)\n",
    "        return prevdA \n",
    "```\n",
    "\n",
    "The gradients are stored in the layer and can be used later by the update function, which performs the gradient descent step. This backward passes chain together for each layer as we will see later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WCvZ8OwYnaZ"
   },
   "source": [
    "The final required function is the update function, which performs the gradient descent step. Nothing spectacular but it expects a learning rate.\n",
    "\n",
    "```python\n",
    "    def update(self, learning_rate):\n",
    "        self.weights = self.weights - learning_rate * self.dW\n",
    "        self.bias = self.bias - learning_rate * self.db\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tYYtmXeYnaZ"
   },
   "source": [
    "While not required, these next function help in printing the model\n",
    "\n",
    "```python    \n",
    "    def __repr__(self):\n",
    "        act = 'none' if self.activation == '' else self.activation\n",
    "        return f'Dense layer (nx={self.weights.shape[1]}, nh={self.weights.shape[0]}, activation={act})'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8s37ly0YnaZ"
   },
   "source": [
    "### The complete DenseLayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYdB_I4XYnaZ"
   },
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \"\"\"\n",
    "    A class to define fully connected layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputDimension, units, activation='', randomMultiplier=0.01):\n",
    "        \"\"\"\n",
    "        Constructor:\n",
    "          inputDimension: number of input features\n",
    "          units: number of neurons in the layer\n",
    "          activation: activation function applied to layer\n",
    "            - options: 'sigmoid', 'tanh', 'relu', ''\n",
    "          randomMultiplier: multiplier applied to the random weights during initialization\n",
    "        \"\"\"\n",
    "        self.weights, self.bias = self.initialize(inputDimension, units, randomMultiplier)\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = activation\n",
    "            self.activationForward = self.sigmoid\n",
    "            self.activationBackward = self.sigmoidGrad\n",
    "        elif activation == 'relu':\n",
    "            self.activation = activation\n",
    "            self.activationForward = self.relu\n",
    "            self.activationBackward = self.reluGrad\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = activation\n",
    "            self.activationForward = self.tanh\n",
    "            self.activationBackward = self.tanhGrad\n",
    "        elif activation != '':\n",
    "            raise ActivationDoesNotExist\n",
    "        else:\n",
    "            self.activation = 'none'\n",
    "            self.activationForward = self.linear\n",
    "            self.activationBackward = self.linear\n",
    "    \n",
    "    def initialize(self, nx, nh, randomMultiplier):\n",
    "        \"\"\"\n",
    "        Initializes weights randomly:\n",
    "          nx: number of input features\n",
    "          nh: number of units\n",
    "          randomMultiplier: multiplier applied to the random weights during initialization\n",
    "        returns:\n",
    "          weights: the randomly initialized weights\n",
    "          bias: the bias terms\n",
    "        \"\"\"\n",
    "        weights = randomMultiplier * np.random.randn(nh, nx)\n",
    "        bias = np.zeros([nh, 1])\n",
    "        return weights, bias\n",
    "\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        \"\"\"\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "        return A\n",
    "        \n",
    "    def sigmoidGrad(self, dA):\n",
    "        \"\"\"\n",
    "        Differential of sigmoid function with chain rule applied\n",
    "        \"\"\"\n",
    "        s = 1 / (1 + np.exp(-self.prevZ))\n",
    "        dZ = dA * s * (1 - s)\n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    def relu(self, Z):\n",
    "        \"\"\"\n",
    "        Relu activation function\n",
    "        \"\"\"\n",
    "        A = np.maximum(0, Z)\n",
    "        return A\n",
    "        \n",
    "    def reluGrad(self, dA):\n",
    "        \"\"\"\n",
    "        Differential of relu function with chain rule applied\n",
    "        \"\"\"\n",
    "        s = np.maximum(0, self.prevZ)\n",
    "        dZ = (s>0) * 1 * dA\n",
    "        return dZ \n",
    "\n",
    "        \n",
    "    def tanh(self, Z):\n",
    "        \"\"\"\n",
    "        Tanh activation function\n",
    "        \"\"\"\n",
    "        A = np.tanh(Z)\n",
    "        return A\n",
    "\n",
    "    def tanhGrad(self, dA):\n",
    "        \"\"\"\n",
    "        Differential of tanh function with chain rule applied\n",
    "        \"\"\"\n",
    "        s = np.tanh(self.prevZ)\n",
    "        dZ = (1 - s**2) * dA\n",
    "        return dZ\n",
    "\n",
    "\n",
    "    def linear(self, Z):\n",
    "        \"\"\"\n",
    "        Placeholder when no activation function is used\n",
    "        \"\"\"\n",
    "        return Z\n",
    "        \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forward pass through layer\n",
    "          A: input vector\n",
    "        \"\"\"\n",
    "        Z = np.dot(self.weights, A) + self.bias\n",
    "        self.prevZ = Z\n",
    "        self.prevA = A\n",
    "        A = self.activationForward(Z)\n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward pass through layer\n",
    "          dA: previous gradient\n",
    "        \"\"\"\n",
    "        dZ = self.activationBackward(dA)\n",
    "        m = self.prevA.shape[1]\n",
    "        self.dW = 1 / m * np.dot(dZ, self.prevA.T)\n",
    "        self.db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        prevdA = np.dot(self.weights.T, dZ)\n",
    "        return prevdA\n",
    "    \n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update weights using gradients from backward pass\n",
    "          learning_rate: the learning rate used in the gradient descent\n",
    "        \"\"\"\n",
    "        self.weights = self.weights - learning_rate * self.dW\n",
    "        self.bias = self.bias - learning_rate * self.db\n",
    "\n",
    "        \n",
    "    def outputDimension(self):\n",
    "        \"\"\"\n",
    "        Returns the output dimension for the next layer\n",
    "        \"\"\"\n",
    "        return len(self.bias)\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Used to print a pretty summary of the layer\n",
    "        \"\"\"\n",
    "        act = 'none' if self.activation == '' else self.activation\n",
    "        return f'Dense layer (nx={self.weights.shape[1]}, nh={self.weights.shape[0]}, activation={act})'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUniQiGeYnaZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjDFwaYqYnaZ"
   },
   "source": [
    "### Alright, one down and one to go.\n",
    "Next we will create a class that will combine the layers. It will also hold the loss function and has to calculate the gradient of the loss. For convenience, we will also add a wrapper to add layers and add a way to pretty print our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpp-8fHAYnaZ"
   },
   "source": [
    "First we start again with the constructor, which has two options, the loss function to be used, and the randomMultiplier used for new layers. The loss function are again created in a function reference and are called using wrapper functions.\n",
    "\n",
    "The model starts with no layers (empty).\n",
    "\n",
    "```python\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, loss='cross-entropy', randomMultiplier = 0.01):\n",
    "        self.layers=[]\n",
    "        self.randomMultiplier = randomMultiplier\n",
    "        if loss=='cross-entropy':\n",
    "            self.lossFunction = self.crossEntropyLoss\n",
    "            self.lossBackward = self.crossEntropyLossGrad\n",
    "        elif loss=='mean-square-error':\n",
    "            self.lossFunction = self.meanSquareError\n",
    "            self.lossBackward = self.meanSquareErrorGrad\n",
    "        else:\n",
    "            raise LossFunctionNotDefined\n",
    "        self.loss=loss\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbcHcWDWYnaa"
   },
   "source": [
    "The next function is a helper function to add layers to the model. You need to give the inputDimension, i.e. the number of input features of the first layer. For the second and further, it will look for the previous layer and use that as the input dimension.\n",
    "\n",
    "You have to specify the number of neurons in the layer (units) and which activation function to use.\n",
    "\n",
    "```python\n",
    "    def addLayer(self, inputDimension=None, units=1, activation=''):\n",
    "        if (inputDimension is None):\n",
    "            if (len(self.layers)==0):\n",
    "                raise InputDimensionNotCorrect\n",
    "            inputDimension=self.layers[-1].outputDimension()\n",
    "        layer = DenseLayer(inputDimension, units, activation, randomMultiplier= self.randomMultiplier)\n",
    "        self.layers.append(layer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RrD_LoxYnaa"
   },
   "source": [
    "Next we define the loss functions and their differentials. Feel free to check if the differentials are correct. the cost method is a wrapper to cal the cost in the training loop later.\n",
    "\n",
    "```python\n",
    "    def crossEntropyLoss(self, Y, A, epsilon=1e-15):\n",
    "        m = Y.shape[1]\n",
    "        loss = -1 * (Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
    "        cost = 1 / m * np.sum(loss)\n",
    "        return np.squeeze(cost)\n",
    "            \n",
    "    def crossEntropyLossGrad(self, Y, A):\n",
    "        dA = -(np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "        return dA\n",
    "    \n",
    "    \n",
    "    def meanSquareError(self, Y, A):\n",
    "        loss = np.square(Y - A)\n",
    "        m = Y.shape[1]\n",
    "        cost = 1 / m * np.sum(loss)\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def meanSquareErrorGrad(self, Y, A):\n",
    "        dA = -2 * (Y - A)\n",
    "        return dA\n",
    "\n",
    "    \n",
    "    def cost(self, Y, A):\n",
    "        return self.lossFunction(Y, A)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD0CgvgbYnaa"
   },
   "source": [
    "The forward, backward, and update method are quite similar as they loop over the layers:\n",
    "\n",
    "```python\n",
    "    def forward(self, X):\n",
    "        x = np.copy(X)\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "            \n",
    "    \n",
    "    def backward(self, A, Y):\n",
    "        dA = self.lossBackward(Y, A)\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "    \n",
    "    \n",
    "    def update(self, learning_rate=0.01):\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "```\n",
    "\n",
    "the backward pass first hast to calculate the first dA, which is de gradient between the activation and the real values. This is then looped through the layers to calculate all gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkZF67zdYnaa"
   },
   "source": [
    "The last two methods are again for pretty printing and one for trainable parameter counting. Nothing special."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSfKvJ8rYnaa"
   },
   "source": [
    "### The complete NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjQSvkftYnaa"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Neural Network structure that holds our layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss='cross-entropy', randomMultiplier = 0.01):\n",
    "        \"\"\"\n",
    "        Constructor:\n",
    "          loss: the loss function. Two are defined:\n",
    "             - 'cross-entropy' and 'mean-square-error'\n",
    "          randomMultiplier: multiplier applied to the random weights during initialization\n",
    "        \"\"\"\n",
    "        self.layers=[]\n",
    "        self.randomMultiplier = randomMultiplier\n",
    "        if loss=='cross-entropy':\n",
    "            self.lossFunction = self.crossEntropyLoss\n",
    "            self.lossBackward = self.crossEntropyLossGrad\n",
    "        elif loss=='mean-square-error':\n",
    "            self.lossFunction = self.meanSquareError\n",
    "            self.lossBackward = self.meanSquareErrorGrad\n",
    "        else:\n",
    "            raise LossFunctionNotDefined\n",
    "        self.loss=loss\n",
    "\n",
    "\n",
    "    def addLayer(self, inputDimension=None, units=1, activation=''):\n",
    "        \"\"\"\n",
    "        Adds a Dense layer to the network:\n",
    "          inputDimension: required when it is the first layer. otherwise takes dimensions of previous layer.\n",
    "          units: number of neurons in the layer\n",
    "          activation: activation function: valid choices are: 'sigmoid', 'tanh', 'relu', ''\n",
    "        \"\"\"\n",
    "        if (inputDimension is None):\n",
    "            if (len(self.layers)==0):\n",
    "                raise InputDimensionNotCorrect\n",
    "            inputDimension=self.layers[-1].outputDimension()\n",
    "        layer = DenseLayer(inputDimension, units, activation, randomMultiplier= self.randomMultiplier)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def crossEntropyLoss(self, Y, A, epsilon=1e-15):\n",
    "        \"\"\"\n",
    "        Cross Entropy loss function\n",
    "          Y: true labels\n",
    "          A: final activation function (predicted labels)\n",
    "          epsilon: small value to make minimize chance for log(0) error\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        loss = -1 * (Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
    "        cost = 1 / m * np.sum(loss)\n",
    "        return np.squeeze(cost)\n",
    "            \n",
    "    def crossEntropyLossGrad(self, Y, A):\n",
    "        \"\"\"\n",
    "        Cross Entropy loss Gradient\n",
    "          Y: true labels\n",
    "          A: final activation function (predicted labels)\n",
    "        \"\"\"\n",
    "        dA = -(np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "        return dA\n",
    "    \n",
    "    \n",
    "    def meanSquareError(self, Y, A):\n",
    "        \"\"\"\n",
    "        Mean square error loss function\n",
    "          Y: true labels\n",
    "          A: final activation function (predicted labels)\n",
    "        \"\"\"\n",
    "        loss = np.square(Y - A)\n",
    "        m = Y.shape[1]\n",
    "        cost = 1 / m * np.sum(loss)\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def meanSquareErrorGrad(self, Y, A):\n",
    "        \"\"\"\n",
    "        Mean square error loss gradient\n",
    "          Y: true labels\n",
    "          A: final activation function (predicted labels)\n",
    "        \"\"\"\n",
    "        dA = -2 * (Y - A)\n",
    "        return dA\n",
    "\n",
    "    \n",
    "    def cost(self, Y, A):\n",
    "        \"\"\"\n",
    "        Cost function wrapper\n",
    "          Y: true labels\n",
    "          A: final activation function (predicted labels)\n",
    "        \"\"\"\n",
    "        return self.lossFunction(Y, A)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the whole model.\n",
    "          X: input vector\n",
    "        \"\"\"\n",
    "        x = np.copy(X)\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "            \n",
    "    \n",
    "    def backward(self, A, Y):\n",
    "        \"\"\"\n",
    "        backward pass through the whole model\n",
    "          Y: true labels\n",
    "          A: final activation function (predicted labels)\n",
    "        \"\"\"\n",
    "        dA = self.lossBackward(Y, A)\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "    \n",
    "    \n",
    "    def update(self, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Update weights and do a step of gradient descent for the whole model.\n",
    "          learning_rate: learning_rate to use\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty print the model\n",
    "        \"\"\"\n",
    "        layrepr = ['  ' + str(ix+1)+' -> ' + str(x) for ix, x in enumerate(self.layers)]\n",
    "        return '[\\n' + '\\n'.join(layrepr) + '\\n]'\n",
    "   \n",
    "    \n",
    "    def numberOfParameters(self):\n",
    "        \"\"\"\n",
    "        Print number of trainable parameters in the model\n",
    "        \"\"\"\n",
    "        n = 0\n",
    "        for layer in self.layers:\n",
    "            n += np.size(layer.weights) + len(layer.bias)\n",
    "        print(f'There are {n} trainable parameters in the model.')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggh2LaiwYnab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPsr5O_CYnab"
   },
   "source": [
    "Alright, the classes are done. Now we need to put these classes to a test. While it looks quite ordered, a small error in one of the differentials can make our whole system useless. Therefore, we will test in small steps in the next section. Looking forward for the spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MWe60pQYnab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw8S1-8LYnab"
   },
   "source": [
    "## 1. Does it do what it is supposed to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qIYXx9VYnab"
   },
   "source": [
    "Nothing is more ennoying than waiting for nothing. Therefore, it is important to first test the easy things instead of training for hours to see that you made a mistake in the loss function. Let us repeat the experiments from last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGDZUNq0Ynab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guiYfD2PYnab"
   },
   "source": [
    "### Linear regression: 25 all over again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-4wqNarYnac"
   },
   "source": [
    "We created an array with a couple of input values $X$. Next we supplied $Y$ using the super duper complicated formula $y = 2x + 1$. In Numpy this looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8QguaNmQK8o"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwPq6Dk8Ynac"
   },
   "outputs": [],
   "source": [
    "X = np.arange(-2, 5, 1).reshape([1, 7]) # Columns as examples\n",
    "Y = 2 * X + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQJQ60yrYnac"
   },
   "source": [
    "Now we will build our model which will try to find our difficult formula and match the true value of 25. For this we will initiate our fresh class and set the loss to mean-square-error. Next we will add a single layer, with a single neuron and no activation function. Lets also try our pretty print :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9oBvjh0iYnac",
    "outputId": "aeab2552-d7c3-427b-83ff-2276ac3f009e"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "model = NeuralNetwork(loss='mean-square-error')\n",
    "model.addLayer(inputDimension=1, units=1, activation='')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxLEd262Ynac"
   },
   "source": [
    "Now we will need our training loop again. It will look very familiar to our logistic regression model from last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3QrXRjoYnac",
    "outputId": "f3f81326-02a9-44cb-ca7c-dfb85bce6c60"
   },
   "outputs": [],
   "source": [
    "num_iterations = 1000\n",
    "for ix in range(num_iterations):\n",
    "    A = model.forward(X)\n",
    "    model.backward(A, Y)\n",
    "    model.update()\n",
    "    if ix % 100 == 0:\n",
    "        print('cost:', model.cost(Y, A)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdAIEiusYnac"
   },
   "source": [
    "The cost decreases as expected and due to the long training the value is close to machine precision. Would we again find 25 when we use the forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVBDmEHKYnac",
    "outputId": "917987ad-0175-44f7-8e70-cc2de49471f0"
   },
   "outputs": [],
   "source": [
    "model.forward(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIyoz_XPYnac"
   },
   "source": [
    "Of course we do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usfq0Yk_Ynad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5tZrqrYYnad"
   },
   "source": [
    "### A go at our Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WduTB6rNYnad"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYQ-sdC3Ynad"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/titanic/train_data.csv')\n",
    "df = df.drop(['Unnamed: 0', 'PassengerId'], axis=1)\n",
    "Y = df['Survived'].to_numpy().reshape([1, -1])\n",
    "X = df.iloc[:,1:].to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ankU5ItgYnad",
    "outputId": "03df8a60-9b80-44a0-fa46-2c60784b6d8a"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtAjMP9ZYnad",
    "outputId": "1cad1887-b982-47ee-8b1b-e48c425fcf39"
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTx3KGp8Ynad"
   },
   "source": [
    "We need to change the model to have 14 input features and of course the sigmoid activation function. The loss function will be cross-entropy, which is the default, so we do not need to specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9nNSPKuYnad",
    "outputId": "0697bc90-dab7-45a3-8bed-03b0d27238a4"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.addLayer(inputDimension=14, units=1, activation='sigmoid')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goC62oqsYnae"
   },
   "source": [
    "Last time, we had an train accuracy of 80%. Can we match this as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQnG-K6bYnae"
   },
   "outputs": [],
   "source": [
    "def roundValue(A):\n",
    "    return np.uint8( A > 0.5)\n",
    "\n",
    "def accuracy(yhat, Y):\n",
    "    return round(np.sum(yhat==Y) / len(yhat.flatten()) * 1000) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9BHPKlIYnae",
    "outputId": "0e99ab9c-6231-4bef-9417-83e377927b85"
   },
   "outputs": [],
   "source": [
    "num_iterations = 8000\n",
    "for ix in range(num_iterations):\n",
    "    A = model.forward(X)\n",
    "    model.backward(A, Y)\n",
    "    model.update()\n",
    "    if ix % 1000 == 0:\n",
    "        yhat = roundValue(A)\n",
    "        print('cost:', model.cost(Y, A), f'\\taccuracy: {accuracy(yhat, Y)}%')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhwpjVLgYnae"
   },
   "source": [
    "Of course we can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUrkSv1vSaRQ"
   },
   "source": [
    "### Q1: We will now implement customization via Keras as the examples above are specific to their respective datasets and are for example only to work through for learning purposes only. **They will not properly run if customized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "qwAdEE9sSwUH",
    "outputId": "3aa24fc7-6527-4259-d2c1-3f87c64bb7e1"
   },
   "outputs": [],
   "source": [
    "# Prerequisite library imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Let's reimport our data\n",
    "df = pd.read_csv('./data/titanic/train_data.csv')\n",
    "X = df.drop(['Unnamed: 0', 'PassengerId'], axis=1)\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)\n",
    "\n",
    "# Let's initialize our model\n",
    "model = Sequential() # Initialising the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVC85pIxYnae"
   },
   "source": [
    "### 1.1) Above we use the Keras libraries to build the model. Here we want to implement a form of scaling to your data either through minmax normalization or z-score standardization using the sklearn.preprocessing libraries (MinMaxScalar or StandardScaler). Justify why you chose one over the other. If the data is already scaled, you still must choose to normalize or standardize. (10 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4f1E7_QnuJE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ru-Sg5jYnae"
   },
   "source": [
    "### 1.2) Using the example for traindata above create a model using different activation functions by setting MYACTIVATIONFXN: (10 points) \n",
    "\n",
    "```python\n",
    "# Hint! Start with model.add(Dense(units = 16, activation = 'relu', input_dim = ?))\n",
    "# Make sure the input_dim parameter is set to the number of features in your X matrix.\n",
    "MYACTIVATIONFXN = 'SOMEFXN'\n",
    "model.add(Dense(units = 14, activation = MYACTIVATIONFXN, input_dim = ?))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6D4qvTGYnaf"
   },
   "outputs": [],
   "source": [
    "# If you decide to initially use a signmoid, make sure the number of units matches the number of targets\n",
    "# in this case we only have 1 target so for sigmoid you need to set units to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfInuSI-UH0O"
   },
   "source": [
    "### Now lets compile our model using the function compile\n",
    "### Here we will use rmsprop as an optimizer and binary crossentropy as our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPF064_rUIGk"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xXvT24DUlRF"
   },
   "source": [
    "### Here we will run our ANN using the fit function using a batch size of 1 and 10 epochs (In the examples above iterations is used instead of epochs). Feel free to play with batch_size and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELg4LXeYUpdx",
    "outputId": "989026b5-9ae9-4167-8253-7e530ce28380"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train.astype('float'), y_train, batch_size = 1, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AOBK9hbYnaf"
   },
   "source": [
    "### 1.3) How does the error (in terms of precission and recall) differ between your model and the example? Write in one paragraph or less how the error differs and why. (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fP8EYlBUYnaf",
    "outputId": "91980151-1ba1-4663-fd2d-6da5a9846006"
   },
   "outputs": [],
   "source": [
    "# Hint! Use the predict function and threshold your results. 0.5 is reasonable.\n",
    "# Please see the BCC jupyter notebook to see how to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfKWyWNHYnaf"
   },
   "source": [
    "## 2) Complex fit of flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_PTyVq4Ynaf"
   },
   "source": [
    "The cool stuf starts with more complex functions. The [Deep learning course from Andrew Ng](https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning) show a way to predict [Rose-functions](https://en.wikipedia.org/wiki/Rose_(mathematics)) using a model with multiple nodes. Lets try that as well!\n",
    "\n",
    "First we need to import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMMal0QFYnaf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('./data/rose/rose.npz')\n",
    "X, Y = data['X'], data['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo5iuacCYnaf"
   },
   "source": [
    "To give a feel how it looks, we will first plot the rose, which has 7 petals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od7DDx7YYnaf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "kuZmpnJAYnag",
    "outputId": "07ab16be-d4dd-4dde-e17b-d2f697d8957a"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "plt.scatter(X[0,:], X[1,:], c=Y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVKVjDw3Ynag"
   },
   "source": [
    "We discussed before that a logistic regression model, like we have defined in our previous example, can only divide in a line. Lets try this out, as the data is clearly not defined in a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4JRKEVSYnag",
    "outputId": "b0195645-5e4d-48d6-aba4-c456db4e8351"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.addLayer(inputDimension=2, units=1, activation='sigmoid')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tJDZBn4Ynag",
    "outputId": "6fa48bc9-d608-4386-9b1c-a2e32f08760f"
   },
   "outputs": [],
   "source": [
    "num_iterations = 1000\n",
    "for ix in range(num_iterations):\n",
    "    A = model.forward(X)\n",
    "    model.backward(A, Y)\n",
    "    model.update(learning_rate=1.2)\n",
    "    if ix % 1000 == 0:\n",
    "        print('cost:', model.cost(Y, A)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q37ZURKIYnag"
   },
   "source": [
    "A short piece of code to generate a contour plot and our Rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "UiiPh-czYnag",
    "outputId": "2a0d60ac-a9b3-47f6-dc4f-ca8e8f0fffd1"
   },
   "outputs": [],
   "source": [
    "def testModel(X, Y, h=0.1, f=1.05):\n",
    "    r = X.max()\n",
    "    xmesh, ymesh = np.meshgrid(np.arange(-r*f, r*f+h, h), np.arange(-r*f, r*f+h, h))\n",
    "    Z = model.forward(((np.c_[xmesh.ravel(), ymesh.ravel()]).T))\n",
    "    Z = (Z > 0.5) * 1\n",
    "    Z = Z.T.reshape(xmesh.shape)\n",
    "    plt.contourf(xmesh, ymesh, Z, cmap=plt.cm.OrRd)\n",
    "    plt.scatter(X[0, :], X[1, :], c=Y.flatten(), cmap=plt.cm.OrRd)\n",
    "testModel(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpUgbGGdYnag"
   },
   "source": [
    "Definitely not a great fit and clearly a line. Now lets add another layer with four units to the model, with the last layer being the same sigmoid layer. The activation function for this model we keep similar to the one from Andrew, which was a tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8Gg9vGBYnah",
    "outputId": "0de97367-6daa-4661-dd79-f470b215923c"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.addLayer(inputDimension=2, units=4, activation='tanh')\n",
    "model.addLayer(units=1, activation='sigmoid')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AST-GxmYYnah",
    "outputId": "9882c353-6017-49bf-be1e-1275325a10f4"
   },
   "outputs": [],
   "source": [
    "num_iterations = 4000\n",
    "for ix in range(num_iterations):\n",
    "    A = model.forward(X)\n",
    "    model.backward(A, Y)\n",
    "    model.update(learning_rate=1.2)\n",
    "    if ix % 1000 == 0:\n",
    "        print('cost:', model.cost(Y, A)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ksAoyXP1Ynah",
    "outputId": "2ab00574-703b-4ff6-f147-33af04f3f594"
   },
   "outputs": [],
   "source": [
    "testModel(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o_C8CDbYnah"
   },
   "source": [
    "This is something that really amazes me. Just a tiny layer more and we have the power to learn this much more complex function. Really great stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJYtVl06a1Fu"
   },
   "source": [
    "### Q2: We will now implement customization via Keras as the examples above are specific to their respective datasets and are for example only to work through for learning purposes only. They will not properly run if customized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "cZF8N-qpbJ-M",
    "outputId": "c2fe06ae-fc4b-4b1a-baec-12e1f2c80eb7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('./data/rose/rose.npz')\n",
    "X, y = data['X'].transpose(), data['Y'].transpose()\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)\n",
    "\n",
    "# Let's initialize our model\n",
    "model = Sequential() # Initialising the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaSeK8fpYnah"
   },
   "source": [
    "### 2.1) Using the example above, try different number of nodes(units) and different activation functions. How does your loss change? (10 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUa1-L50Ynah",
    "outputId": "5d08f4cd-60cd-4ac1-d30a-2b829cb038fb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVKN7RMLYnah"
   },
   "source": [
    "### 2.2) Calculate your new error for 2 different models using classification report. Also, using the metrics, explain why you see the same or why you see a different error. (10 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvi8QHpjYnah",
    "outputId": "fcbecff5-a292-49f6-cad8-f5d34b52d908"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvcIw3pHYnah"
   },
   "source": [
    "### 2.3) Choose your best model! Now plot the new results using the plotting example shown above but using our newly trained best/coolest model. (10 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK14EFkPYnai"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRRH5UW1Ynai"
   },
   "source": [
    "## 3) Cats vs not cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfHJ4U5UYnai"
   },
   "source": [
    "In the same course from Andrew, they pointed out that there is a terrible shortage of cat-detectors on the internet. Therefore, our class should detect cats in style! Lets download the same dataset. For details, please check out Andrew's Course, which is big fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "1oU-QdGGYnai",
    "outputId": "c7617fb4-ac6d-4d33-ea05-9b623c26c923"
   },
   "outputs": [],
   "source": [
    "data = np.load('./data/cats/cats.npz')\n",
    "X_train, y_train = data['Xtrain'], data['Ytrain']\n",
    "X_test, y_test = data['Xtest'], data['Ytest']\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z94ZPG6MYnai"
   },
   "source": [
    "Same functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMvK3YERYnaj"
   },
   "outputs": [],
   "source": [
    "def roundValue(A):\n",
    "    return np.uint8( A > 0.5)\n",
    "\n",
    "def accuracy(yhat, Y):\n",
    "    return round(np.sum(yhat==Y) / len(yhat.flatten()) * 1000) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6vjLs7pYnaj"
   },
   "source": [
    "The model will be the same as the two layer model from the course. first layer has 7 units and a Relu activation function. Second layer is the sigmoid cat/no-cat layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8at1sjWEYnaj",
    "outputId": "2d70c526-414c-42ee-bf07-c7c4ed99e6e6"
   },
   "outputs": [],
   "source": [
    "nx = X_train.shape[0]\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.addLayer(inputDimension=nx, units=7, activation='relu')\n",
    "model.addLayer(units=1, activation='sigmoid')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FiDOXq2rYnaj",
    "outputId": "8fb5f83a-60c2-48c8-a38a-936665036a21"
   },
   "outputs": [],
   "source": [
    "num_iterations = 1000\n",
    "for ix in range(1, num_iterations+1):\n",
    "    A = model.forward(X_train)\n",
    "    model.backward(A, y_train)\n",
    "    model.update(learning_rate=0.03)\n",
    "    if ix % 100 == 0:\n",
    "        yhat_test = roundValue(A)\n",
    "        print('cost:', model.cost(y_train, A), f'\\taccuracy: {accuracy(yhat_test, y_train)}%')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjVsxQrMYnak"
   },
   "source": [
    "Amazing accuracy. But we of course all know we should check this with a proper test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIYb-ebSYnak",
    "outputId": "3f42c701-1737-4651-d749-6fc38c1f0425"
   },
   "outputs": [],
   "source": [
    "A = model.forward(X_test)\n",
    "yhat_test = roundValue(A)\n",
    "acc = accuracy(yhat_test, y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv1MjXhlYnak"
   },
   "source": [
    "An accuracy of 74% is definitely not bad for such a simple model which only works on pixel values. For the fun of it, lets look at some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "L0exh7oXYnak",
    "outputId": "2d1a1106-d94b-46ad-aa79-b85412143bd4"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "imgs = X_test.T.reshape([50, 64, 64, 3])\n",
    "\n",
    "fig, ax = plt.subplots(1, n, figsize=(16,8))\n",
    "for ix in range(n):\n",
    "    num = np.random.randint(imgs.shape[0])\n",
    "    ax[ix].imshow(imgs[num])\n",
    "    if yhat.flatten()[num] == 0:\n",
    "        ax[ix].set_title('This is clearly not a cat')\n",
    "    else:\n",
    "        ax[ix].set_title('Yup! it\\'s a cat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNymHxiNiY4S"
   },
   "source": [
    "### Q3: Your turn! Let's do this!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "UVrE_ZFuiarQ",
    "outputId": "77c4f5c3-7ff4-4e0d-c392-f71a65b7e498"
   },
   "outputs": [],
   "source": [
    "data = np.load('./data/cats/cats.npz')\n",
    "X_train, y_train = data['Xtrain'].transpose(), data['Ytrain'].transpose()\n",
    "X_test, y_test = data['Xtest'].transpose(), data['Ytest'].transpose()\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)\n",
    "\n",
    "# Let's initialize our model\n",
    "model = Sequential() # Initialising the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrCvQs6NYnak"
   },
   "source": [
    "### 3.1) Same as before, build a new model with different number of hidden layers, nodes and activation functions. Describe reason for any similarity or difference (30 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmdATv6tYnak"
   },
   "outputs": [],
   "source": [
    "# Try using different iterations using a simple layout like above. 10, 100, 1000 epochs. \n",
    "# What happens with your loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGUrbX9ysgJJ"
   },
   "outputs": [],
   "source": [
    "# Try using different layers and activation function with different number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVYjQT00stLx"
   },
   "outputs": [],
   "source": [
    "# What happens when you add convolutional layers? What happens to our training loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmp8LC30Ynak"
   },
   "source": [
    "### 3.2) Calculate your accuracy (10 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7xQi21-Ynak",
    "outputId": "f6f8b73e-52d5-4d29-87c6-fb0955b0227c"
   },
   "outputs": [],
   "source": [
    "# Hint! Use the predict function and threshold your results. 0.5 is reasonable.\n",
    "# In your classification report since we are only predicting cats you will need to set the parameter labels\n",
    "# labels=np.unique(yhat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf_Khwq1Ynak"
   },
   "source": [
    "### 3.3) Calculate your precision and recall manually (10 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8BpIv03Ynal"
   },
   "outputs": [],
   "source": [
    "# Recall calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BriMkolotghk"
   },
   "outputs": [],
   "source": [
    "# Precision calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3E_cvZbkb_o"
   },
   "source": [
    "### Let's plot!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "rq71-Hdnke1W",
    "outputId": "54e48a05-5b23-4dbc-fc30-56d622f1f4b3"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "imgs = X_test.reshape([50, 64, 64, 3])\n",
    "fig, ax = plt.subplots(1, n, figsize=(16,8))\n",
    "for ix in range(n):\n",
    "    num = np.random.randint(imgs.shape[0])\n",
    "    ax[ix].imshow(imgs[num])\n",
    "    if yhat_test[num] == 0:\n",
    "        ax[ix].set_title('This is clearly not a cat')\n",
    "    else:\n",
    "        ax[ix].set_title('Yup! it\\'s a cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC4SnF_7Ynal"
   },
   "source": [
    "## Round up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ba-yJKQIYnal"
   },
   "source": [
    "I hope you all had fun, writing your own ANN. In my opinon, writing these things from the ground up is the best way to learn how it actually works. I hope that you see that these systems are not magical, but simple matrix multiplications, unfortunately just a very lot of them. The most difficult part is of course the back propagation, where we need to calculate the gradients. Our simple ANNs are quite doable, but adding more different layers to them, can make it a bit more cumbersome. Still the essence is very similar to what we have done today.\n",
    "\n",
    "My suggestion is to play around with these structures, rewrite parts of them, or even better, write your own from scratch!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJkBdpHDYnal"
   },
   "source": [
    "Please let me know if you have any comments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oo5nI67mYnal"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUZ99lNyYnal"
   },
   "source": [
    "## Apendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pc0AoxY_Ynal"
   },
   "source": [
    "### Generating Rose Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNcaQgjQYnal"
   },
   "outputs": [],
   "source": [
    "def generateRoseData():\n",
    "    k=7\n",
    "    pointPerPetal = 100\n",
    "    cutOff = 0.1\n",
    "    r = 4\n",
    "\n",
    "    theta = np.linspace(0,np.pi, pointPerPetal * k)\n",
    "    xx = r * np.cos(k * theta) * np.cos(theta)\n",
    "    yy = r * np.cos(k * theta) * np.sin(theta)\n",
    "    cc = [np.ones(pointPerPetal) if ix % 3 == 0 else np.zeros(pointPerPetal) for ix in np.arange(k)]\n",
    "    cc = np.roll(np.hstack(cc).astype(np.uint8), -pointPerPetal//2)\n",
    "    x = xx[(xx**2 + yy**2)**0.5 > cutOff]\n",
    "    y = yy[(xx**2 + yy**2)**0.5 > cutOff]\n",
    "    col = cc[(xx**2 + yy**2)**0.5 > cutOff]\n",
    "    X = np.vstack([x,y])\n",
    "    Y = np.copy(col).reshape([1, -1])\n",
    "    return X, Y\n",
    "X, Y = generateRoseData()\n",
    "np.savez_compressed('./data/rose/rose.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rq4YAjRgYnam"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBlwWwKQYnam"
   },
   "source": [
    "### Processing Andrews CatvNotCat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lkHWbzVYnam"
   },
   "outputs": [],
   "source": [
    "# If you get an error here, install h5py via pip3 install h5py\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUyH9EvUYnam"
   },
   "outputs": [],
   "source": [
    "# Data downloaded from:\n",
    "# https://github.com/ridhimagarg/Cat-vs-Non-cat-Deep-learning-implementation\n",
    "def processCatData():\n",
    "    train_dataset = h5py.File(\"./data/cats/train_catvnoncat.h5\", mode='r')\n",
    "    Xtrain = np.array(train_dataset[\"train_set_x\"])\n",
    "    Y_train = np.array(train_dataset[\"train_set_y\"])\n",
    "    test_dataset = h5py.File(\"./data/cats/test_catvnoncat.h5\", mode='r')\n",
    "    Xtest = np.array(test_dataset[\"test_set_x\"])\n",
    "    Y_test = np.array(test_dataset[\"test_set_y\"])\n",
    "    X_train = Xtrain / 255\n",
    "    X_test = Xtest / 255\n",
    "    X_train = X_train.reshape(209, -1).T\n",
    "    Y_train = Y_train.reshape(-1, 209)\n",
    "    X_test = X_test.reshape(50, -1).T\n",
    "    Y_test = Y_test.reshape(-1, 50)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "Xtrain, Xtest, Ytrain, Ytest = processCatData()\n",
    "np.savez_compressed('./data/cats/cats.npz', Xtrain=Xtrain, Xtest=Xtest, Ytrain=Ytrain, Ytest=Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkplsgolYnam"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy2XhQSoYnan"
   },
   "source": [
    "# Credits\n",
    "#### Edwin Solares - Conversion to google colab, conversion to Keras and preprocessing data to work with Kears\n",
    "#### Dennis Bakhuis - May the Fourth (be with you) 2020\n",
    "https://linkedin.com/in/dennisbakhuis/ \\\n",
    "https://github.com/dennisbakhuis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0ja7ns_Ynan"
   },
   "source": [
    "### Exercise 4 - Collaborative Statement (5 points) \n",
    "#### You must fill this out even if you worked alone to get credit.\n",
    "\n",
    "It is mandatory to include a Statement of Collaboration in each submission, that follows the guidelines below.\n",
    "Include the names of everyone involved in the discussions (especially in-person ones), and what was discussed.\n",
    "All students are required to follow the academic honesty guidelines posted on the course website. For\n",
    "programming assignments in particular, I encourage students to organize (perhaps using Piazza) to discuss the\n",
    "task descriptions, requirements, possible bugs in the support code, and the relevant technical content before they\n",
    "start working on it. However, you should not discuss the specific solutions, and as a guiding principle, you are\n",
    "not allowed to take anything written or drawn away from these discussions (no photographs of the blackboard,\n",
    "written notes, referring to Piazza, etc.). Especially after you have started working on the assignment, try to restrict\n",
    "the discussion to Piazza as much as possible, so that there is no doubt as to the extent of your collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gwkjn5yQYnan"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "z0ja7ns_Ynan"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a89676b4e6eb606cbd593327750625970cd1436e4503cc9fa5aaeefa410b226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
